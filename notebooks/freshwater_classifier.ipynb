{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the quality of water - Intel oneAPI Hackathon 2023 - Sandy Inspires\n",
    "\n",
    "## What's in the notebook\n",
    "- Importing Packages\n",
    "- Loading Dataset\n",
    "- Understanding Dataset\n",
    "- Preprocessing stage\n",
    "- Balancing Dataset\n",
    "- Model Training\n",
    "- Model Evaluation\n",
    "- Hyper-parameter Tunning\n",
    "\n",
    "`Note: There has been a lot of snippets and POCs made to make this entire things work which is not included as part of this notebook (it's too extensive)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intel packages and configurations\n",
    "from modin.config import Engine\n",
    "Engine.put(\"dask\")\n",
    "from dask.distributed import Client\n",
    "client = Client(n_workers=6)\n",
    "\n",
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user library\n",
    "import numpy as np\n",
    "#import pandas_profiling\n",
    "import random as rnd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch up sklearn\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import category_encoders as ce\n",
    "\n",
    "# machine learning model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report, RocCurveDisplay, PrecisionRecallDisplay, ConfusionMatrixDisplay, auc, roc_curve\n",
    "\n",
    "# plots\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_water_df = pd.read_csv(\"datasets/dataset.csv\", nrows=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_water_df = pd.read_csv(\"datasets/encoded_20K_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_water_df = pd.read_csv(\"datasets/20k_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_water_df = pd.read_csv(\"datasets/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_water_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't use drop na since the difference is too high - total data size 5956843 - 3981800 after drop\n",
    "#fresh_water_df = fresh_water_df.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_water_df[\"Target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_water_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = fresh_water_df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr[\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numeric Columns - {list(fresh_water_df._get_numeric_data(axis=1).columns)}\")\n",
    "print(f\"Categorical Columns - {list(fresh_water_df.select_dtypes(include=object).columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_water_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_water_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"Month\"]\n",
    "fresh_water_df = fresh_water_df.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"Color\", \"Month\", \"Source\", \"Index\"]\n",
    "fresh_water_df = fresh_water_df.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_columns_to_use = [\"Iron\", \"Nitrate\", \"Chloride\", \"Turbidity\", \"Odor\", \"Sulfate\", \"Chlorine\", \"Target\"]\n",
    "feature_columns_to_use = [\"Iron\", \"Nitrate\", \"Chloride\", \"Turbidity\", \"Odor\", \"Sulfate\", \"Chlorine\", \"Target\"]\n",
    "fresh_water_df = fresh_water_df[feature_columns_to_use]\n",
    "numerical_columns_to_fill_median = list(fresh_water_df._get_numeric_data(axis=0).columns)\n",
    "print(\"Numeric Values\", numerical_columns_to_fill_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling NaNs with median and rounding it to 10 since the floating points are too large to handle\n",
    "for column_name in numerical_columns_to_fill_median:\n",
    "    column_median = fresh_water_df[column_name].median()\n",
    "    fresh_water_df[column_name] = fresh_water_df[column_name].replace(np.NaN, column_median).round(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_water_df = pd.get_dummies(fresh_water_df, prefix=[\"clr\"], columns=[\"Color\"]).drop(columns=[\"Index\", \"Source\",\"Month\"])\n",
    "#fresh_water_df = pd.get_dummies(fresh_water_df, prefix=[\"clr\", \"src\", \"mon\"], columns=[\"Color\", \"Source\",\"Month\"]).drop(columns=[\"Index\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1_count = fresh_water_df.loc[fresh_water_df[\"Target\"] == 1][\"Target\"].count()\n",
    "target_0_count = fresh_water_df.loc[fresh_water_df[\"Target\"] == 0][\"Target\"].count()\n",
    "print(target_1_count, target_0_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_target_count = min(target_1_count, target_0_count)\n",
    "print(min_target_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get equal number of target values to train\n",
    "ndf = fresh_water_df.query(\"Target== 1\")[:min_target_count].append(\n",
    "    fresh_water_df.query(\"Target == 0\")[:min_target_count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = ndf.drop(columns=[\"Target\"]), ndf[\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "x = scaler.fit_transform(X)\n",
    "scaled_x = pd.DataFrame(x, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    scaled_x, Y, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=20, max_depth = 13, min_samples_leaf=4)\n",
    "#random_forest = RandomForestClassifier(max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=60)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "y_pred = random_forest.predict(X_test)\n",
    "print(\n",
    "      f\"Training Accuracy - {round(random_forest.score(X_train, Y_train) * 100, 2)}\")\n",
    "print(\n",
    "    f\"Testing Accuracy - {round(random_forest.score(X_test, Y_test) * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=100)\n",
    "logreg.fit(X_train, Y_train)\n",
    "print(\n",
    "      f\"Training Accuracy - {round(logreg.score(X_train, Y_train) * 100, 2)}\")\n",
    "print(\n",
    "    f\"Testing Accuracy - {round(logreg.score(X_test, Y_test) * 100, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "Y_pred = svc.predict(X_test)\n",
    "\n",
    "print(\n",
    "      f\"Training Accuracy - {round(svc.score(X_train, Y_train) * 100, 2)}\")\n",
    "print(\n",
    "    f\"Testing Accuracy - {round(svc.score(X_test, Y_test) * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "train_x = np.asarray(X_train)\n",
    "train_y = np.asarray(Y_train)\n",
    "test_x = np.asarray(X_test)\n",
    "test_y = np.asarray(Y_test)\n",
    "\n",
    "tf.random.set_seed(45)\n",
    "model_3 = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),    \n",
    "\n",
    "    tf.keras.layers.Dropout(0.2),  \n",
    "\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),    \n",
    "\n",
    "    tf.keras.layers.Dropout(0.5),  \n",
    "\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "\n",
    "    tf.keras.layers.Dropout(0.6),\n",
    "    \n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "\n",
    "                optimizer=tf.keras.optimizers.Adam(lr=0.005),\n",
    "\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_3.fit(train_x, train_y, epochs=50, verbose=1, batch_size=64, use_multiprocessing =True, shuffle=True, steps_per_epoch=10)\n",
    "\n",
    "model_3.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, Y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = xgb_cl.predict(X_test)\n",
    "\n",
    "print(\n",
    "      f\"Training Accuracy - {round(xgb_cl.score(X_train, Y_train) * 100, 2)}\")\n",
    "print(\n",
    "    f\"Testing Accuracy - {round(xgb_cl.score(X_test, Y_test) * 100, 2)}\")\n",
    "\n",
    "target_names = [\"unsafe\", \"safe\"]\n",
    "print(classification_report(Y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"safe\", \"unsafe\"]\n",
    "print(classification_report(Y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_mat = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=con_mat,display_labels=random_forest.classes_)\n",
    "disp.plot(cmap=\"summer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(Y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='example estimator')\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(random_forest, \"models/97_random_forest_nor_full.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'bootstrap': [True, False],\n",
    "              'max_depth': list(range(1,100)),\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'n_estimators': [10, 20, 30, 40]\n",
    "              }\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, n_jobs=-1, return_train_score=True)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(RandomForestClassifier(),\n",
    "                                   param_grid)\n",
    "random_search.fit(X_train, Y_train)\n",
    "print(random_search.best_estimator_)\n",
    "\n",
    "h_random_forest = random_search.best_estimator_\n",
    "h_random_forest.fit(X_train, Y_train)\n",
    "print(\n",
    "      f\"Training Accuracy - {round(h_random_forest.score(X_train, Y_train) * 100, 2)}\")\n",
    "print(\n",
    "    f\"Testing Accuracy - {round(h_random_forest.score(X_test, Y_test) * 100, 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5ae7862e32e22ab9c4f6d8d84d57ed0e940331a2f1237e7583effdb7c55abcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
